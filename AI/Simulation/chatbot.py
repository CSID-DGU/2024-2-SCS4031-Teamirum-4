from openai import OpenAI
import streamlit as st
import json
import os
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import streamlit.components.v1 as components
#OCR
import pdfplumber
from PIL import Image
import pytesseract

# OpenAI API í‚¤ ì„¤ì •
pytesseract.pytesseract.tesseract_cmd = r"C:/Program Files/Tesseract-OCR/tesseract.exe"
 
# ì¶”ì²œ ê²°ê³¼ë¥¼ JSON íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°
# ìƒëŒ€ ê²½ë¡œë¡œ JSON íŒŒì¼ ê²½ë¡œ ì„¤ì •
pdf_dir = '/Users/jjrm_mee/Desktop/2024-2-SCS4031-Teamirum-4/recommendations.json'

# íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
if not os.path.exists(pdf_dir):
    raise FileNotFoundError(f"File not found: {pdf_dir}")

with open(pdf_dir, 'r', encoding='utf-8') as f:
    recommendation_results = json.load(f)

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'[^\w\sã„±-ã…ã…-ã…£ê°€-í£.,!?]', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return text.strip()

def create_prompt(user_input, recommendation_results):
    rag_results = []
    for rec in recommendation_results:
        try:
            rag_results.append(
                f"Rank {rec.get('rank', 'N/A')}: {rec.get('summary_text', 'ë‚´ìš© ì—†ìŒ')} "
                f"(ìƒí’ˆëª…: {rec.get('product_name', 'ìƒí’ˆëª… ì—†ìŒ')}, "
                f"ìœ ì‚¬ë„ ì ìˆ˜: {rec.get('similarity_score', 0.0):.2f})"
            )
        except KeyError as e:
            rag_results.append(f"ë°ì´í„° ëˆ„ë½: {e}")
    
    references_text = "\n".join(rag_results)

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±
    system_message = (
        f"ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.\n"
        f"ë‹¤ìŒì€ ê´€ë ¨ ì°¸ê³ ìë£Œì…ë‹ˆë‹¤:\n{references_text}"
    )
    return system_message

# GPT ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def ask_gpt(user_input, recommendation_results):
    terms_dir = "/Users/jjrm_mee/Desktop/2024-2-SCS4031-Teamirum-4/ìƒí’ˆìš”ì•½ì„œ/ì‹¤ì†ë³´í—˜" 
    #terms_dir = os.path.join(os.path.dirname(__file__), "ìƒí’ˆìš”ì•½ì„œ1", "ì‹¤ì†ë³´í—˜")
 
    print(terms_dir)
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„±
    system_message = create_prompt(user_input, recommendation_results)
    
    
    context = "ì¶”ì²œëœ ë³´í—˜ìƒí’ˆ ëª©ë¡ê³¼ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤:\n"
    
    # RAG ê²°ê³¼ë¥¼ ëª…í™•íˆ êµ¬ì¡°í™”
    rag_results = []
    for idx, rec in enumerate(recommendation_results[:3]):
        product_name = rec.get('product_name', 'ìƒí’ˆëª… ì—†ìŒ')
        relevant_text = rec.get('relevant_text', 'ê´€ë ¨ í…ìŠ¤íŠ¸ ì—†ìŒ')
        similarity_score = rec.get('similarity_score', 0.0)
        rag_results.append({
            "rank" : idx+1,
            "product_name" : product_name,
            "relevant_text": relevant_text,
            "similarity_score": similarity_score,
        })
       
         # ì•½ê´€ íŒŒì¼ ì²˜ë¦¬ ë° RAG ê²°ê³¼ ë³´ê°•
        for result in rag_results:
            terms_filename = result["product_name"]  # íŒŒì¼ ì´ë¦„ê³¼ ì•½ê´€ íŒŒì¼ëª…ì´ ì¼ì¹˜ì‹œì¼œì•¼í•¨
            terms_path = os.path.join(terms_dir, terms_filename)
            relevant_text = ""
        
        if os.path.exists(terms_path):
            full_text = ""
            print("ìˆë‚˜ìš©")
            try:
                # ì•½ê´€ íŒŒì¼ ë¡œë“œ
                if terms_path.endswith('.pdf'):
                    with pdfplumber.open(terms_path) as pdf:
                        for page in pdf.pages:
                            page_text = page.extract_text()
                            if page_text:
                                full_text += page_text + " "
                elif terms_path.endswith('.txt'):
                    with open(terms_path, 'r', encoding='utf-8') as f:
                        full_text = f.read()
                else:
                    print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {terms_filename}")
                    full_text = ""
                
                # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
                full_text = clean_text(full_text)
                sentences = re.split(r'(?<=[.!?])\s+', full_text)[:1000]# ë¬¸ì¥ ìˆ˜ ì œí•œ

                # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥ ì°¾ê¸°
                vectorizer = TfidfVectorizer().fit(sentences)
                sentence_embeddings = vectorizer.transform(sentences)
                user_embedding = vectorizer.transform([user_input])
                
                similarities = cosine_similarity(user_embedding, sentence_embeddings)
                most_similar_idx = similarities.argsort()[0][-1]
                most_similar_sentence = sentences[most_similar_idx]
                
                relevant_text = most_similar_sentence
            except Exception as e:
                print(f"{terms_filename} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                relevant_text = "í•´ë‹¹ ìƒí’ˆì˜ ì•½ê´€ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        else:
            relevant_text = "í•´ë‹¹ ìƒí’ˆì˜ ì•½ê´€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        result['relevant_text'] = relevant_text

    # ë©”ì‹œì§€ êµ¬ì„± (ChatCompletion API í˜•ì‹)
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë³´í—˜ ìƒí’ˆì— ëŒ€í•œ ì „ë¬¸ê°€ë¡œì„œ ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."},
        {"role": "user", "content": user_input},
        {"role": "assistant", "content": context},
        {"role": "user", "content": "ìœ„ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."}
    ]

    # GPT í˜¸ì¶œ
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages,
            max_tokens=1000,
            temperature=0.7,
            n=1,
            stop=None,
        )
        answer = response.choices[0].message.content.strip()
        return answer
    except Exception as e:
        return f"Error: {str(e)}"
    

# OCR ì²˜ë¦¬ í•¨ìˆ˜
def ocr_image_to_text(image):
    try:
        text = pytesseract.image_to_string(image, lang='kor')
        return clean_text(text)
    except Exception as e:
        return f"Error during OCR processing: {str(e)}"

# ğŸŸ¢ğŸŸ¢ğŸŸ¢í•´ì‹œíƒœê·¸ ì¶”ì¶œí•¨ìˆ˜
def extract_hashtags(raw_content):
    okt = Okt()
    nouns = okt.nouns(raw_content)
    unique_nouns = list(set(nouns))
    hashtags = " ".join([f"#{noun}" for noun in unique_nouns])
    return hashtags

# í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ì„¤ì •
# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ëŒ€í™”í˜• ì±—ë´‡", layout="wide")

# ì „ì²´ ë ˆì´ì•„ì›ƒ
#col1, col2 = st.columns([1, 1])  # ì™¼ìª½ ì‚¬ì´ë“œë°”(1)ì™€ ì˜¤ë¥¸ìª½ ë©”ì¸ ì±—ë´‡(2) ë¹„ìœ¨

# **ì‚¬ì´ë“œë°”**: ì¶”ì²œ ë³´í—˜ ì¶œë ¥
#with col1:
   

#ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ
recommendation_results_sorted = sorted(
    recommendation_results,
    key=lambda x: x.get("similarity_score", 0.0),
    reverse=True
)

#ì‚¬ì´ë“œë°” ì œëª©
st.sidebar.markdown(
    "<div style='font-size:20px; font-weight:bold; text-align:center;'>ì¶”ì²œëœ ë³´í—˜ Top 3</div><hr>",
    unsafe_allow_html=True
)

# ì¶”ì²œ ê²°ê³¼ ì¶œë ¥
for idx,rec in enumerate(recommendation_results):
    product_name = rec.get("product_name", "ìƒí’ˆëª… ì—†ìŒ").replace(".pdf", "")
    similarity_score = rec.get("similarity_score", 0.0)
    reason = rec.get("reason", "")

    # ê´„í˜¸ ì•ˆì˜ ë‚´ìš© ì¶”ì¶œ ë° ì¤‘ë³µ ì œê±°
    if "(" in reason and ")" in reason:
        raw_content = reason[reason.find("(") + 1:reason.find(")")]  # ê´„í˜¸ ì•ˆ ì¶”ì¶œ
        hashtags =  extract_hashtags(raw_content)
    else:
        hashtags = "#ì¶”ì²œì´ìœ  ì—†ìŒ"
    print("hashtags: ", hashtags)

    # ë²”ì£¼í™” ë° ì‹ í˜¸ë“± ìƒ‰ìƒ ì•„ì´ì½˜ ì„¤ì •
    if idx == 0:
        category = "ë§¤ìš° ì í•©"
        icon = "ğŸŸ¢"  # ì´ˆë¡ ì‹ í˜¸ë“±
        font_color = "black"
    elif idx < 3:
        category = "ì í•©"
        icon = "ğŸŸ "  # ì£¼í™© ì‹ í˜¸ë“±
        font_color = "black"
    else:
        break  # ìƒìœ„ 3ê°œê¹Œì§€ë§Œ í‘œì‹œ
    
    # ì‚¬ì´ë“œë°”ì— í‘œì‹œ
    st.sidebar.markdown(
        f"<div style='font-size:18px; color:{font_color}; font-weight:bold;'>"
        f"<b>{product_name}</b></div>", 
        unsafe_allow_html=True
    )
    st.sidebar.markdown(f"**í‰ê°€**: {icon} ({category})", unsafe_allow_html=False)

    st.sidebar.markdown(
        f"<p style = 'font-size:14px; color:gray;'>ì¶”ì²œ ì´ìœ : {hashtags}</p>",
        unsafe_allow_html=True
    )
    # êµ¬ë¶„ì„  ì¶”ê°€
    st.sidebar.markdown("<hr>", unsafe_allow_html=True)


# **ë©”ì¸ ì˜ì—­**: ì±—ë´‡ UI
#with col2:
    # ì œëª©
st.markdown(
    """
    <div style='text-align:center; font-size:30px; font-weight:bold;'>
        í‹°ë¯¸ë£¸ ë³´í—˜ ì±—ë´‡ ğŸ‘‹
    </div>
    """, 
    unsafe_allow_html=True
)
# ğŸŸ¢ğŸŸ¢ğŸŸ¢st ë³€ê²½

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ëŒ€í™” ì´ë ¥ ê´€ë¦¬
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."}
    ]
# íŒŒì¼ ì—…ë¡œë“œ UI
uploaded_file = st.file_uploader("ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (PNG, JPG)", type=["png", "jpg", "jpeg"])
user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

# OCR ì²˜ë¦¬
ocr_text = ""
if uploaded_file:
    image = Image.open(uploaded_file)
    with st.spinner("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ ì¤‘..."):
        ocr_text = ocr_image_to_text(image)
    st.success("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")  

 # OCR ë°ì´í„° + ì‚¬ìš©ì ì…ë ¥ ê²°í•©
#combined_input = f"{user_input.strip()} {ocr_text.strip()}".strip()

    
# ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
if user_input:
    assistant_response = ask_gpt(user_input, recommendation_results)
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.session_state.messages.append({"role": "assistant", "content": assistant_response})

    with st.chat_message("user"):
        st.markdown(user_input)
    with st.chat_message("assistant"):
        st.markdown(assistant_response)
else:
    st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
