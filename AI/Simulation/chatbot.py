import streamlit as st
import json
import os
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import streamlit.components.v1 as components
#OCR
import pdfplumber
from PIL import Image
import pytesseract
import openai
from konlpy.tag import Okt
import unicodedata

# OpenAI API í‚¤ ì„¤ì •
openai.api_key=('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')

# ì¶”ì²œ ê²°ê³¼ ë¡œë“œ
recommendationstest_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../recommendations.json'))
with open(recommendationstest_path, 'r', encoding='utf-8') as f:
    recommendation_results = json.load(f)

# ì¹´í…Œê³ ë¦¬ ë¡œë“œ
recommendation_category_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../recommendations_category.json'))
with open(recommendation_category_path, 'r', encoding='utf-8') as f:
    recommendation_category = json.load(f)

# ì§„ë£Œë¹„ ë°ì´í„° ë¡œë“œ
with open(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ì§„ë£Œë¹„_ê²°ê³¼.json')), 'r', encoding='utf-8') as f:
    fee_data = json.load(f)

# ì•½ê´€, ìš”ì•½ì„œ ë””ë ‰í† ë¦¬ ì„¤ì • (í•„ìš”ì— ë§ê²Œ ê²½ë¡œ ìˆ˜ì •)
# terms_dir = "/Users/ddinga/Downloads/ì•½ê´€ì‹¤ì†ë³´í—˜"
# summary_dir = "/Users/ddinga/Downloads/ìš”ì•½ì„œì‹¤ì†ë³´í—˜"

pdf_summary_dirs = {
    "ì‹¤ì†ë³´í—˜": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆìš”ì•½ì„œ/ì‹¤ì†ë³´í—˜')),
    "ê±´ê°•ë³´í—˜": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆìš”ì•½ì„œ/ê±´ê°•ë³´í—˜(ì•” ë“±)')),
    "ì¢…ì‹ ë³´í—˜": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆìš”ì•½ì„œ/ì¢…ì‹ ë³´í—˜')),
    "ì •ê¸°ë³´í—˜": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆìš”ì•½ì„œ/ì •ê¸°ë³´í—˜')),
    "ê¸°íƒ€": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆìš”ì•½ì„œ/ê¸°íƒ€')),
}

pdf_full_dirs = {
    "ì‹¤ì†ë³´í—˜": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆì•½ê´€/ì‹¤ì†ë³´í—˜')),
    "ê±´ê°•ë³´í—˜": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆì•½ê´€/ê±´ê°•ë³´í—˜(ì•” ë“±)')),
    "ì¢…ì‹ ë³´í—˜": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆì•½ê´€/ì¢…ì‹ ë³´í—˜')),
    "ì •ê¸°ë³´í—˜": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆì•½ê´€/ì •ê¸°ë³´í—˜')),
    "ê¸°íƒ€": os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìƒí’ˆì•½ê´€/ê¸°íƒ€')),
}

# print(recommendation_results[0])
pdf_summary_dir = pdf_summary_dirs.get(recommendation_category[0])
pdf_full_dir = pdf_full_dirs.get(recommendation_category[0])

terms_dir = pdf_full_dir
summary_dir = pdf_summary_dir

def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # ê³µë°± ì œê±°
    text = re.sub(r'[^\w\sã„±-ã…ã…-ã…£ê°€-í£.,!?]', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return text.strip()

def extract_relevant_text(pdf_text, keywords=None, max_sentences=10):
    if keywords is None:
        keywords = ["ê³„ì‚°", "ë³´í—˜ê¸ˆ", "ê³µì œ", "í™˜ê¸‰", "ë³´ìƒê¸ˆì•¡", "ê³µì œê¸ˆì•¡", "ë³´ìƒë¹„ìœ¨", "ìê¸°ë¶€ë‹´ê¸ˆ"]
    sentences = re.split(r'(?<=[.!?])\s+', pdf_text)
    relevant_sentences = [s for s in sentences if any(k in s for k in keywords)]
    return " ".join(relevant_sentences[:max_sentences])

def find_calculation_logic(product_name, summary_dir):
    if not product_name.endswith(".pdf"):
        terms_path = os.path.join(summary_dir, f"{product_name}.pdf")
    else:
        terms_path = os.path.join(summary_dir, product_name)
    
    if not os.path.exists(terms_path):
        return f"ì•½ê´€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {terms_path}"

    pdf_text = extract_text_from_pdf(terms_path)
    pdf_text = clean_text(pdf_text)
    relevant_text = extract_relevant_text(pdf_text)

    prompt = f"""
    ì•„ë˜ëŠ” {product_name} ìƒí’ˆì˜ ìš”ì•½ì„œ ë‚´ìš©ì…ë‹ˆë‹¤.
    ì´ ë³´í—˜ìƒí’ˆì€ ì‹¤ì†ì˜ë£Œë¹„ë³´í—˜ìœ¼ë¡œ, ë³´ì¥ëŒ€ìƒ ì˜ë£Œë¹„ì— ëŒ€í•˜ì—¬ ì¼ì • ê¸ˆì•¡ ë˜ëŠ” ì¼ì • ë¹„ìœ¨ì˜ ìê¸°ë¶€ë‹´ê¸ˆì„ ê³µì œí•œ ë’¤ ë³´í—˜ê¸ˆì„ ì‚°ì¶œí•©ë‹ˆë‹¤.
    ì•½ê´€ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ë³´í—˜ê¸ˆ ê³„ì‚° ë¡œì§(ìê¸°ë¶€ë‹´ê¸ˆ ê³„ì‚°, ê³µì œê¸ˆì•¡, ë³´ìƒë¹„ìœ¨, ì‚°ì¶œ ë°©ì‹ ë“±)ì„ ëª…í™•íˆ ì œì‹œí•´ ì£¼ì„¸ìš”.

    [ë°˜í™˜ í˜•ì‹ ì•ˆë‚´]
    - ê°€ëŠ¥í•˜ë‹¤ë©´ ìˆ˜ì‹ í˜•íƒœ(ì˜ˆì‹œ): ë³´í—˜ê¸ˆ = ë³´ìƒëŒ€ìƒ ì˜ë£Œë¹„ - max(ê³µì œê¸ˆì•¡, ë³´ìƒëŒ€ìƒ ì˜ë£Œë¹„ * ë³´ìƒë¹„ìœ¨)
    - ìœ„ í˜•ì‹ëŒ€ë¡œ ëª…í™•íˆ í‘œí˜„í•˜ê¸° ì–´ë µë‹¤ë©´, ìê¸°ë¶€ë‹´ê¸ˆ ì°¨ê° ë°©ì‹ê³¼ ë³´ìƒë¹„ìœ¨ì„ ì„¤ëª… ë¬¸ì¥ í˜•íƒœë¡œë¼ë„ ì œì‹œí•´ì£¼ì„¸ìš”.
    - ë³´ìƒëŒ€ìƒ ì˜ë£Œë¹„, ê³µë‹¨ë¶€ë‹´ì´ì•¡, ì´ë¯¸ë‚©ë¶€í•œê¸ˆì•¡ ë“±ì˜ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²½ìš° ë³€ìˆ˜ëª…ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.

    ìš”ì•½ì„œ ë‚´ìš©:
    {relevant_text}
    """
    try:
        response = openai.ChatCompletion.create(
            messages = [{"role": "user", "content": prompt}],
            model = "gpt-3.5-turbo",
        )
        return response["choices"][0]["message"]["content"].strip()
    except Exception as e:
        return f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}"

def extract_additional_data(product_name, relevant_text):
    base_product_name = product_name.replace(".pdf", "").strip()
    base_product_name = unicodedata.normalize('NFC', base_product_name)

    data = {
        "ë³´ìƒëŒ€ìƒì˜ë£Œë¹„": 0,
        "ë³´ìƒë¹„ìœ¨": 0,
        "ìê¸°ë¶€ë‹´ê¸ˆ": 0
    }

    target_name_1 = unicodedata.normalize('NFC', "ì‚¼ì„±ìƒëª…-ë…¸í›„ì‹¤ì†ì˜ë£Œë¹„ë³´ì¥ë³´í—˜(ê°±ì‹ í˜•,ë¬´ë°°ë‹¹)")
    target_name_2 = unicodedata.normalize('NFC', "ì‚¼ì„±ìƒëª…-ê°„í¸ì‹¤ì†ì˜ë£Œë¹„ë³´ì¥ë³´í—˜(ê¸°ë³¸í˜•,ê°±ì‹ í˜•,ë¬´ë°°ë‹¹)")
    target_name_3 = unicodedata.normalize('NFC', "êµë³´ìƒëª…-ì‹¤ì†ì˜ë£Œë¹„ë³´í—˜(ê°±ì‹ í˜•)â…¢[ê³„ì•½ì „í™˜ìš©]")

    if base_product_name == target_name_3:
        data["ë³´ìƒëŒ€ìƒì˜ë£Œë¹„"] = 27130
        data["ë³´ìƒë¹„ìœ¨"] = 0.2
        data["ìê¸°ë¶€ë‹´ê¸ˆ"] = 10000
    elif base_product_name == target_name_2:
        data["ë³´ìƒëŒ€ìƒì˜ë£Œë¹„"] = 27130
        data["ë³´ìƒë¹„ìœ¨"] = 0.3
        data["ìê¸°ë¶€ë‹´ê¸ˆ"] = 20000
    elif base_product_name == target_name_1:
        data["ë³´ìƒëŒ€ìƒì˜ë£Œë¹„"] = 27130
        data["ë³´ìƒë¹„ìœ¨"] = 0.8
        data["ìê¸°ë¶€ë‹´ê¸ˆ"] = 30000

    return data

def calculate_reimbursement(product_name, formula, fee_data, additional_data):
    """
    ìƒí’ˆëª…ì— ë”°ë¼ ê³ ì •ëœ ë¡œì§ìœ¼ë¡œ ê³„ì‚°
    í•­ìƒ: ë³´í—˜ê¸ˆ = ë³´ìƒëŒ€ìƒì˜ë£Œë¹„ - max(ìê¸°ë¶€ë‹´ê¸ˆ, ë³´ìƒëŒ€ìƒì˜ë£Œë¹„*ë³´ìƒë¹„ìœ¨)
    """
    ë³´ìƒëŒ€ìƒì˜ë£Œë¹„ = additional_data["ë³´ìƒëŒ€ìƒì˜ë£Œë¹„"]
    ë³´ìƒë¹„ìœ¨ = additional_data["ë³´ìƒë¹„ìœ¨"]
    ìê¸°ë¶€ë‹´ê¸ˆ = additional_data["ìê¸°ë¶€ë‹´ê¸ˆ"]

    ë³´í—˜ê¸ˆ = ë³´ìƒëŒ€ìƒì˜ë£Œë¹„ - max(ìê¸°ë¶€ë‹´ê¸ˆ, ë³´ìƒëŒ€ìƒì˜ë£Œë¹„ * ë³´ìƒë¹„ìœ¨)
    return ë³´í—˜ê¸ˆ

# ê¸°ì¡´ ì±—ë´‡ ì½”ë“œ í•¨ìˆ˜
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'[^\w\sã„±-ã…ã…-ã…£ê°€-í£.,!?]', '', text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°
    return text.strip()

def ask_gpt(user_input, recommendation_results):
    # ì˜ìˆ˜ì¦ ë˜ëŠ” íŠ¹ì • ìƒí’ˆëª… "êµë³´ìƒëª…-ì‹¤ì†ì˜ë£Œë¹„ë³´í—˜(ê°±ì‹ í˜•)â…¢[ê³„ì•½ì „í™˜ìš©]"ì´ í¬í•¨ë˜ë©´ 
    # ì²« ë²ˆì§¸ ë¡œì§(ë³´í—˜ê¸ˆ ê³„ì‚°)ì„ ì ìš©
    if "ì˜ìˆ˜ì¦" in user_input:
        return handle_receipt_logic(recommendation_results)
    elif "ì‹¤ì†ì˜ë£Œë¹„ë³´í—˜" in user_input:
        return handle_specific_product_logic("êµë³´ìƒëª…-ì‹¤ì†ì˜ë£Œë¹„ë³´í—˜(ê°±ì‹ í˜•)â…¢[ê³„ì•½ì „í™˜ìš©]")
    else:
        # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸°ì¡´ ë¡œì§ ì‹¤í–‰
        # terms_dir = "/Users/ddinga/Downloads/ì•½ê´€ì‹¤ì†ë³´í—˜"
        # terms_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../ìš”ì•½ì„œì‹¤ì†ë³´í—˜'))
        context = "ì•„ë˜ëŠ” ì¶”ì²œëœ ë³´í—˜ ìƒí’ˆ ëª©ë¡ê³¼ ê´€ë ¨ ë‚´ìš©ì…ë‹ˆë‹¤:\n"
        for idx, rec in enumerate(recommendation_results):
            product_name = rec.get('product_name', 'ìƒí’ˆëª… ì—†ìŒ')
            terms_filename = product_name  
            terms_path = os.path.join(terms_dir, terms_filename)
            relevant_text = ""

            if os.path.exists(terms_path):
                full_text = ""
                try:
                    if terms_path.endswith('.pdf'):
                        with pdfplumber.open(terms_path) as pdf:
                            for page in pdf.pages:
                                page_text = page.extract_text()
                                if page_text:
                                    full_text += page_text + " "
                    elif terms_path.endswith('.txt'):
                        with open(terms_path, 'r', encoding='utf-8') as f:
                            full_text = f.read()
                    else:
                        full_text = ""
                    
                    full_text = clean_text(full_text)
                    sentences = re.split(r'(?<=[.!?])\s+', full_text)
                    sentences = sentences[:1000]
                    
                    vectorizer = TfidfVectorizer().fit(sentences + [user_input])
                    sentence_embeddings = vectorizer.transform(sentences)
                    user_embedding = vectorizer.transform([user_input])

                    similarities = cosine_similarity(user_embedding, sentence_embeddings)
                    most_similar_idx = similarities.argmax()
                    window_size = 2  # ì•ë’¤ 2ë¬¸ì¥ì”© ë¬¶ëŠ” ì˜ˆì‹œ (ì ì ˆíˆ ì¡°ì • ê°€ëŠ¥)
                    start_idx = max(0, most_similar_idx - window_size)
                    end_idx = min(len(sentences), most_similar_idx + window_size + 1)
                    relevant_sentences = sentences[start_idx:end_idx]
                    relevant_text = " ".join(relevant_sentences)

                except Exception as e:
                    relevant_text = "í•´ë‹¹ ìƒí’ˆì˜ ì•½ê´€ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            else:
                relevant_text = "í•´ë‹¹ ìƒí’ˆì˜ ì•½ê´€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            rec['relevant_text'] = relevant_text
            context += f"{idx+1}. {product_name}: {relevant_text}\n"
            print("relevant_text")

            
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë³´í—˜ ìƒí’ˆì— ëŒ€í•œ ì „ë¬¸ê°€ë¡œì„œ ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë‹¤ìŒì€ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n" + context},
            {"role": "user", "content": user_input}
        ]
        
        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages,
                max_tokens=500,
                temperature=0.35,
                n=1,
                stop=None,
            )
            answer = response['choices'][0]['message']['content'].strip()
            return answer
        except Exception as e:
            return None

def handle_receipt_logic(recommendation_results):
    # "ì˜ìˆ˜ì¦" í‚¤ì›Œë“œ ìˆì„ ë•Œ ëª¨ë“  ì¶”ì²œìƒí’ˆì— ëŒ€í•´ ê³„ì‚° ë¡œì§ ì ìš©
    result_text = ""
    fixed_calculation_logic = "ë³´í—˜ê¸ˆ = ë³´ìƒëŒ€ìƒì˜ë£Œë¹„ - max(ìê¸°ë¶€ë‹´ê¸ˆ, ë³´ìƒëŒ€ìƒì˜ë£Œë¹„*ë³´ìƒë¹„ìœ¨)"
    for rec in recommendation_results:
        product_name = rec.get("product_name", None)
        if not product_name:
            continue
        
        calculation_logic = find_calculation_logic(product_name, terms_dir)
        additional_data = extract_additional_data(product_name, calculation_logic)
        reimbursement = calculate_reimbursement(product_name, fixed_calculation_logic, fee_data, additional_data)

        result_text += f"**ìƒí’ˆëª…: {product_name}**\n"
        result_text += f"ì¶”ì¶œëœ ê³„ì‚° ë¡œì§(ì›ë¬¸): {calculation_logic}\n"
        result_text += f"ì‹¤ì œ ì ìš©ë˜ëŠ” ê³ ì • ê³„ì‚° ë¡œì§: {fixed_calculation_logic}\n"
        result_text += f"ì ìš©ëœ ë°ì´í„°: {additional_data}\n"
        result_text += f"ê³„ì‚°ëœ ë³´í—˜ê¸ˆ: {reimbursement}ì›\n\n"
    return result_text

def handle_specific_product_logic(product_name):
    # íŠ¹ì • ìƒí’ˆëª…ì— ëŒ€í•´ ê³ ì •ëœ ë¡œì§ ì‹¤í–‰
    calculation_logic = find_calculation_logic(product_name, terms_dir)
    fixed_calculation_logic = "ë³´í—˜ê¸ˆ = ë³´ìƒëŒ€ìƒì˜ë£Œë¹„ - max(ìê¸°ë¶€ë‹´ê¸ˆ, ë³´ìƒëŒ€ìƒì˜ë£Œë¹„*ë³´ìƒë¹„ìœ¨)"
    additional_data = extract_additional_data(product_name, calculation_logic)
    reimbursement = calculate_reimbursement(product_name, fixed_calculation_logic, fee_data, additional_data)

    result_text = f"**ìƒí’ˆëª…: {product_name}**\n"
    result_text += f"ì¶”ì¶œëœ ê³„ì‚° ë¡œì§(ì›ë¬¸): {calculation_logic}\n"
    result_text += f"ì‹¤ì œ ì ìš©ë˜ëŠ” ê³ ì • ê³„ì‚° ë¡œì§: {fixed_calculation_logic}\n"
    result_text += f"ì ìš©ëœ ë°ì´í„°: {additional_data}\n"
    result_text += f"ê³„ì‚°ëœ ë³´í—˜ê¸ˆ: {reimbursement}ì›\n\n"
    return result_text


# í•´ì‹œíƒœê·¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_hashtags(raw_content):
    okt = Okt()
    nouns = okt.nouns(raw_content)
    unique_nouns = list(set(nouns))
    hashtags = " ".join([f"#{noun}" for noun in unique_nouns])
    return hashtags

st.set_page_config(page_title="í‹°ë¯¸ë£¸ ë³´í—˜ ì±—ë´‡", layout="wide")

def load_css(file_name):
    with open(file_name, "r", encoding="utf-8") as f:
        return f"<style>{f.read()}</style>"

st.markdown(load_css("styles.css"), unsafe_allow_html=True)

#ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ
recommendation_results_sorted = sorted(
    recommendation_results,
    key=lambda x: x.get("similarity_score", 0.0),
    reverse=True
)

st.sidebar.markdown(
    """
    <div class="sidebar-container">
        <h2>ì¶”ì²œ ë³´í—˜ TOP 3</h2>
    </div>
    </br>
    """,
    unsafe_allow_html=True,
)

for idx,rec in enumerate(recommendation_results):
    # ìƒí’ˆëª…ê³¼ ìœ ì‚¬ë„ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    product_name = rec.get("product_name", "ìƒí’ˆëª… ì—†ìŒ").replace(".pdf", "")
    similarity_score = rec.get("similarity_score", 0.0)

    # ì¶”ì²œ ì´ìœ ì™€ í‚¤ì›Œë“œ ê°€ì ¸ì˜¤ê¸°
    reason = rec.get("reason", "")
    keywords = rec.get("keywords", ["#ì¶”ì²œì´ìœ  ì—†ìŒ"])  # í‚¤ì›Œë“œê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •

    if keywords:
        hashtags = " ".join(keywords)  # í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ê²°í•©
    else:
        hashtags = "#ì¶”ì²œì´ìœ  ì—†ìŒ"

    # ê²°ê³¼ ì¶œë ¥
    print(f"ìƒí’ˆëª…: {product_name}")
    #print(f"ìœ ì‚¬ë„ ì ìˆ˜: {similarity_score:.2f}")
    #print(f"ì¶”ì²œ ì´ìœ : {reason}")
    print(f"í‚¤ì›Œë“œ: {hashtags}")
    print("-" * 30)

    # ë²”ì£¼í™” ë° ì‹ í˜¸ë“± ìƒ‰ìƒ ì•„ì´ì½˜ ì„¤ì •
    if idx == 0:
        category = "ë§¤ìš° ì í•©"
        icon = "ğŸŸ¢"  # ì´ˆë¡ ì‹ í˜¸ë“±
        font_color = "black"
    elif idx < 3:
        category = "ì í•©"
        icon = "ğŸŸ "  # ì£¼í™© ì‹ í˜¸ë“±
        font_color = "black"
    else:
        break  # ìƒìœ„ 3ê°œê¹Œì§€ë§Œ í‘œì‹œ
    
    # ì™¼ìª½ ì‚¬ì´ë“œë°”ì— í‘œì‹œ
    st.sidebar.markdown(
        f"<div style='font-size:18px; color:{font_color}; font-weight:bold;'>"
        f"<b>{product_name}</b></div>", 
        unsafe_allow_html=True
    )
    st.sidebar.markdown(f"**í‰ê°€**: {icon} ({category})", unsafe_allow_html=False)

    st.sidebar.markdown(
        f"<p style = 'font-size:14px; color:gray;'>ì¶”ì²œ ì´ìœ : {hashtags}</p>",
        unsafe_allow_html=True
    )
    # êµ¬ë¶„ì„  ì¶”ê°€
    st.sidebar.markdown("<hr>", unsafe_allow_html=True)

st.sidebar.markdown(
    """
    <div class="fixed-header header-two">
        <h3>ì±—ë´‡ì—ê²Œ ë¬¼ì–´ë³´ë©´ ì¢‹ì€ ì§ˆë¬¸ LIST !</h3>
        <ul>
            <li>Q. ë³´í—˜ ê°€ì… ì‹œ ê°€ì¥ ì¤‘ìš”í•œ ì ì€?</li>
            <li>Q. ì œ ê¸°ì¤€ì—ì„œ í•´ë‹¹ ë³´í—˜ ê°€ì…ì‹œ ë³´ì¥ ê¸ˆì•¡ì€ ì–¼ë§ˆë‚˜ ë‚˜ì˜¤ë‚˜ìš”?</li>
            <li>Q. ë³´í—˜ê¸ˆì„ ë³´ì¥ë°›ì§€ ëª»í•˜ëŠ” ê²½ìš°ëŠ” ë­ê°€ ìˆë‚˜ìš”?</li>
            <li>Q. ì¶”ì²œë°›ì€ ë³´í—˜ì˜ ì²­êµ¬ ì ˆì°¨ê°€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?</li>
            <li>Q. ì¶”ì²œë°›ì€ ë³´í—˜ì˜ ë³´í—˜ê¸ˆ ì§€ê¸‰ê¸°ì¤€ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?</li>
            <li>Q. ì¶”ì²œë°›ì€ ë³´í—˜ì˜ í•´ì•½í™˜ê¸‰ê¸ˆì„ ì•Œë ¤ì£¼ì„¸ìš”</li>
        </ul>
    </div>
    """,
    unsafe_allow_html=True,
)

st.markdown(
    """
    <div class="header">
        <h1>í‹°ë¯¸ë£¸ ë³´í—˜ ì±—ë´‡ ğŸ‘‹</h1>
    </div>
    """,
    unsafe_allow_html=True,
)

st.markdown("<div style='height: 30px;'></div>", unsafe_allow_html=True)
st.markdown(
    """
    <div style="background-color: #ffffff; padding: 50px; border-radius: 10px; margin: 100px 0;">
    </div>
    """,
    unsafe_allow_html=True,
)

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë³´í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."}
    ]

uploaded_file = st.file_uploader("ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (PNG, JPG)", type=["png", "jpg", "jpeg"])

st.markdown(
    """
    <div style="background-color: #ffffff; padding: 5px; border-radius: 10px; margin: 5px 0;">
    </div>
    """,
    unsafe_allow_html=True,
)

st.markdown("<div style='height: 30px;'></div>", unsafe_allow_html=True)

for message in st.session_state.messages:
    role = message["role"]
    content = message["content"]
    if role == "user":
        with st.chat_message("user"):
            st.markdown(content)
    elif role == "assistant":
        with st.chat_message("assistant"):
            st.markdown(content)

user_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

if user_input:
    assistant_response = ask_gpt(user_input, recommendation_results)
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.session_state.messages.append({"role": "assistant", "content": assistant_response})

    with st.chat_message("user"):
        st.markdown(user_input)
    with st.chat_message("assistant"):
        st.markdown(assistant_response)
else:
    st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

#st.write("Debug: user_input =", user_input)
#st.write("Debug: messages =", st.session_state.messages)

st.markdown(
    """
    <div style="background-color: #ffffff; padding: 20px; border-radius: 10px; margin: 20px 0;">
    </div>
    """,
    unsafe_allow_html=True,
)
st.markdown("<div style='height: 30px;'></div>", unsafe_allow_html=True)

st.markdown(
    """
    <div class="fixed-footer">
        <p>
            Â© 2024 í‹°ë¯¸ë£¸ ë³´í—˜ ì±—ë´‡ | ë¬¸ì˜ ì‚¬í•­ì€ 
            <a href="mailto:contact@teamiroom.com" style="text-decoration:none; color:#007bff;">
                contact@timeroom.com
            </a>ìœ¼ë¡œ ì—°ë½í•˜ì„¸ìš”.
        </p>
    </div>
    """,
    unsafe_allow_html=True,
)

col1, col2, col3, col4, col5, col6, col7, col8,col9,col10,col11,col12,col13,col14,col15,col16,col17 = st.columns(17)

image_1_path = os.path.abspath(os.path.join(os.path.dirname(__file__), './img/receipt.png'))
image_2_path = os.path.abspath(os.path.join(os.path.dirname(__file__), './img/customer-support.png'))
image_3_path = os.path.abspath(os.path.join(os.path.dirname(__file__), './img/insurance-company.png'))
image_4_path = os.path.abspath(os.path.join(os.path.dirname(__file__), './img/qna.png'))
image_5_path = os.path.abspath(os.path.join(os.path.dirname(__file__), './img/heart.png'))
link5 = "https://pub.insure.or.kr/#fsection01"

os.path.abspath(os.path.join(os.path.dirname(__file__), './img/heart.png'))

with col3:
    st.image(image_1_path, caption="ë³´í—˜ê¸ˆ ê³„ì‚°")

with col6:
    st.image(image_2_path, caption="ê³ ê°ì„¼í„° ì „í™”ë²ˆí˜¸")

with col9:
    st.image(image_3_path, caption="ë³´í—˜ì‚¬ í™ˆí˜ì´ì§€")

with col12:
    st.image(image_4_path, caption="ìì£¼ ë¬»ëŠ” ì§ˆë¬¸")

with col15:
    st.image(image_5_path, caption="ìƒëª…ë³´í—˜ ê³µì‹œì‹¤ ë¹„êµ")

col1, col2, col3, col4, col5, col6, col7, col8, col9, col10, col11 = st.columns(11)

with col2:
    with st.expander("ë³´í—˜ê¸ˆ ì •ë³´ ë³´ê¸°"):
        st.write("ë³´í—˜ê¸ˆ ê³„ì‚°ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ì—¬ê¸°ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

with col4:
    with st.expander("ê³ ê°ì„¼í„°"):
        st.write("4ëŒ€ ìƒëª…ë³´í—˜ ê³ ê°ì„¼í„° ì „í™”ë²ˆí˜¸:")
        st.write("ì‚¼ì„±ìƒëª…: 1588-3114")
        st.write("í•œí™”ìƒëª…: 1566-0100")
        st.write("êµë³´ìƒëª…: 1588-1001")
        st.write("ì‹ í•œë¼ì´í”„: 1588-8000")

with col6:
    with st.expander("4ëŒ€ ìƒëª…ë³´í—˜ì‚¬ í™ˆí˜ì´ì§€ ë§í¬"):
        st.write("[ì‚¼ì„±ìƒëª…](https://www.samsunglife.com)")
        st.write("[í•œí™”ìƒëª…](https://www.hanwhalife.com)")
        st.write("[êµë³´ìƒëª…](https://www.kyobo.co.kr)")
        st.write("[ì‹ í•œë¼ì´í”„](https://www.shinhanlife.co.kr)")

with col8:
    st.markdown(
        f'<a href="{link5}" target="_blank" style="text-decoration:none; font-size:16px;">ğŸ‘‰ ìƒëª…ë³´í—˜ ê³µì‹œì‹¤ ë°”ë¡œê°€ê¸°</a>',
        unsafe_allow_html=True,
    )

with col10:
    st.markdown(
        f'<a href="{link5}" target="_blank" style="text-decoration:none; font-size:16px;">ğŸ‘‰ ìƒëª…ë³´í—˜ ê³µì‹œì‹¤ ë°”ë¡œê°€ê¸°</a>',
        unsafe_allow_html=True,
    )
